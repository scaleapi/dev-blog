{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Much code repurposed from the Pytorch tutorial\n",
    "# \"Torchvision Object Detection Finetuning Tutorial\"\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import nucleus\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "import detection.utils as utils\n",
    "import detection.engine as engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'your_nucleus_api_key'\n",
    "DSET_SLICE = 'your_nucleus_dataset_slice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transforms, data):\n",
    "        self.transforms = transforms\n",
    "        self.imgs = data\n",
    "        # self.load_labels()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images\n",
    "        item = self.imgs[idx][\"item\"]\n",
    "        annotations = self.imgs[idx][\"annotations\"]\n",
    "        img_path = \"~/path/to/image/\" + item.metadata.get(\n",
    "            \"filename\", \"Img-3039.jpg\"\n",
    "        )\n",
    "        with open(img_path, \"rb\") as file:\n",
    "            img = Image.open(file).convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        for anno in annotations[\"box\"]:\n",
    "            xmin = anno.x\n",
    "            ymin = anno.y\n",
    "            xmax = anno.x + anno.width\n",
    "            ymax = anno.y + anno.height\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        num_objs = len(boxes)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # labels = torch.as_tensor(label_ids, dtype=torch.int64)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)  # horse (1) or background\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        # must return:\n",
    "        # @image: a PIL Image of size (H, W)\n",
    "        # @target: a dict containing the following fields\n",
    "        #   boxes (FloatTensor[N, 4]): coordinates of N bounding boxes in [x0,y0,x1,y1] format ranging from 0-W and 0-H\n",
    "        #   labels (Int64Tensor[N]): the label for each bounding box, 0 represents background class\n",
    "        #   image_id (Int64Tensor[1]): an image identifier\n",
    "        #   area (Tensor[N]): the area of the bounding box\n",
    "        #   iscrowd (UInt8Tensor[N]): instances with iscrowd=True will be ignored during eval\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Tuple\n",
    "from torch import Tensor, nn\n",
    "from torchvision.transforms import functional\n",
    "from torchvision.transforms import transforms as T\n",
    "\n",
    "def _flip_coco_person_keypoints(kps, width):\n",
    "    flip_inds = [0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15]\n",
    "    flipped_data = kps[:, flip_inds]\n",
    "    flipped_data[..., 0] = width - flipped_data[..., 0]\n",
    "    # Maintain COCO convention that if visibility == 0, then x, y = 0\n",
    "    inds = flipped_data[..., 2] == 0\n",
    "    flipped_data[inds] = 0\n",
    "    return flipped_data\n",
    "\n",
    "\n",
    "class ToTensor(nn.Module):\n",
    "    def forward(\n",
    "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
    "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
    "        image = functional.pil_to_tensor(image)\n",
    "        image = functional.convert_image_dtype(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(T.RandomHorizontalFlip):\n",
    "    def forward(\n",
    "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
    "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = functional.hflip(image)\n",
    "            if target is not None:\n",
    "                width, _ = functional.get_image_size(image)\n",
    "                target[\"boxes\"][:, [0, 2]] = width - target[\"boxes\"][:, [2, 0]]\n",
    "                if \"masks\" in target:\n",
    "                    target[\"masks\"] = target[\"masks\"].flip(-1)\n",
    "                if \"keypoints\" in target:\n",
    "                    keypoints = target[\"keypoints\"]\n",
    "                    keypoints = _flip_coco_person_keypoints(keypoints, width)\n",
    "                    target[\"keypoints\"] = keypoints\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_horse_data():\n",
    "    client = nucleus.NucleusClient(API_KEY)\n",
    "    data = client.get_slice(DSET_SLICE)\n",
    "    ia = data.items_and_annotations()\n",
    "    filtered = list(filter(lambda row: row[\"item\"].metadata.get(\"filename\"), ia))\n",
    "    print(len(filtered))\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    # load model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    num_classes = 2  # 1 class (horse) + background\n",
    "    input_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(input_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs):\n",
    "    raw_data = get_raw_horse_data()\n",
    "    lengths = []\n",
    "    cur_length = len(raw_data)\n",
    "    while cur_length >= 50:\n",
    "        lengths.append(cur_length)\n",
    "        cur_length -= 50\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    print(lengths)\n",
    "    dataset = CustomDataset(get_transform(train=True), raw_data)\n",
    "    dataset_test = CustomDataset(get_transform(train=False), raw_data)\n",
    "    indices = list(range(len(dataset)))\n",
    "    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "    # define data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=utils.collate_fn\n",
    "    )\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, num_workers=4, collate_fn=utils.collate_fn\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    # construct optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    # train for num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        engine.train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        lr_scheduler.step()\n",
    "        if epoch == 4 or epoch == 9 or epoch == 14:\n",
    "            path = f\"./model_checkpoints/model_epoch_{epoch}.pkl\"\n",
    "            with open(path, \"wb+\") as file:\n",
    "                torch.save(model.state_dict(), file)\n",
    "\n",
    "    with open(\"./model_checkpoints/eval_output.log\", \"a+\") as f:\n",
    "        orig_target = sys.stdout\n",
    "        sys.stdout = f\n",
    "        engine.evaluate(model, data_loader_test, device=device)\n",
    "        print(\"=======\\n\")\n",
    "        sys.stdout = orig_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e0188fdb53a59b66768a41e073b7ef59f505fc8e10bb4d9ccdafb4b24a22e7f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('blogpost')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
